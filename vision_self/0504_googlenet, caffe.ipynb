{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d417722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bcc837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "## 사물 분류\n",
    "\n",
    "filename = '../caffe/fig/coffee-6632524_960_720.webp'\n",
    "\n",
    "img = cv2.imread(filename)\n",
    "\n",
    "if img is None:\n",
    "    print('image read failed')\n",
    "    sys.exit()\n",
    "    \n",
    "model = '../caffe/bvlc_googlenet.caffemodel'\n",
    "config = '../caffe/deploy.prototxt'\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('network load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "classNames = []\n",
    "\n",
    "with open('../caffe/classification_classes_ILSVRC2012.txt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "# print(classNames)\n",
    "# print(classNames[1])\n",
    "\n",
    "# blobFromImage(image[, scalefactor[, size[, mean[, swapRB[, crop[, ddepth]]]]]]) -> retval\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (244, 244), (104, 117, 123), swapRB = False)\n",
    "\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "# print(prob)\n",
    "print(prob.shape)\n",
    "\n",
    "out = prob.flatten()\n",
    "# print(out.shape)\n",
    "\n",
    "ClassId = np.argmax(out)\n",
    "# print(ClassId)\n",
    "# print(classNames[ClassId])\n",
    "# print(out[ClassId])\n",
    "\n",
    "confidence = out[ClassId]\n",
    "category = classNames[ClassId]\n",
    "\n",
    "text = f'{category} ({confidence*100:4.2f} %)'\n",
    "\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_COMPLEX,\n",
    "           1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de4d89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 얼굴 인식\n",
    "\n",
    "img = cv2.imread('../facedetection/dog-7058195_960_720.webp')\n",
    "\n",
    "model = '../facedetection/opencv_face_detector_uint8.pb'\n",
    "config = '../facedetection/opencv_face_detector.pbtxt'                    # 모델의 구조. pbtxt가 tensorflow\n",
    "\n",
    "# model = '../facedetection/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "# config = '../facedetection/deploy.prototxt'\n",
    "\n",
    "face_detect_net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if face_detect_net.empty():\n",
    "    print('net load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "blob = cv2.dnn.blobFromImage(img, 1, (300, 300), (104, 177, 123), swapRB = False)\n",
    "\n",
    "face_detect_net.setInput(blob)\n",
    "\n",
    "out = face_detect_net.forward()               # net을 통과하여 반환하라(두 개의 더미 데이터가 포함되어서 나옴\n",
    "# print(out.shape) -> (1, 1, 200, 7)\n",
    "\n",
    "detect = out[0, 0, :, :]\n",
    "# print(detect.shape) -> (200, 7)\n",
    "\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "for i in range(detect.shape[0]):\n",
    "    confidence = detect[i, 2]\n",
    "    \n",
    "    if confidence > 0.5:\n",
    "        x1 = int(detect[i, 3]*w)\n",
    "        y1 = int(detect[i, 4]*h)\n",
    "        x2 = int(detect[i, 5]*w)\n",
    "        y2 = int(detect[i, 6]*h)\n",
    "        \n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        \n",
    "        text = 'Face : {}%'.format(round(confidence*100, 2))\n",
    "        \n",
    "        cv2.putText(img, text, (x1, y1-2), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d3c6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 영상에서 얼굴 인식\n",
    "\n",
    "model = '../facedetection/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "config = '../facedetection/deploy.prototxt'\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('video open failed')\n",
    "    sys.exit()\n",
    "    \n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('net load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print('frame read failed')\n",
    "        break\n",
    "        \n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123))\n",
    "    net.setInput(blob)\n",
    "    out = net.forward()\n",
    "    \n",
    "    detect = out[0, 0, :, :]\n",
    "    \n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i, 2]\n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            x1 = int(detect[i, 3]*w)\n",
    "            y1 = int(detect[i, 4]*h)\n",
    "            x2 = int(detect[i, 5]*w)\n",
    "            y2 = int(detect[i, 6]*h)\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        \n",
    "            text = 'Face : {}%'.format(round(confidence*100, 2))\n",
    "        \n",
    "            cv2.putText(frame, text, (x1, y1-2), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e568fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8537634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "## 직접 학습하여 활용\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# print(x_train.shape) -> (60000, 28, 28)\n",
    "# print(x_train.dtype) -> uint8\n",
    "\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)/255,\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)/255.\n",
    "\n",
    "# print(y_train.shape) -> (60000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe8873bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size = (3, 3), input_shape = (28, 28, 1),\n",
    "                             activation = 'relu'))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size = 2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation = 'relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0a62b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "889d8372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "210/210 [==============================] - 25s 118ms/step - loss: 0.2806 - accuracy: 0.9144 - val_loss: 0.0795 - val_accuracy: 0.9754\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 25s 118ms/step - loss: 0.0771 - accuracy: 0.9772 - val_loss: 0.0578 - val_accuracy: 0.9827\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 25s 121ms/step - loss: 0.0530 - accuracy: 0.9835 - val_loss: 0.0506 - val_accuracy: 0.9841\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 26s 122ms/step - loss: 0.0374 - accuracy: 0.9885 - val_loss: 0.0466 - val_accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 26s 123ms/step - loss: 0.0297 - accuracy: 0.9903 - val_loss: 0.0491 - val_accuracy: 0.9869\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 25s 121ms/step - loss: 0.0253 - accuracy: 0.9916 - val_loss: 0.0480 - val_accuracy: 0.9863\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 26s 121ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.0484 - val_accuracy: 0.9873\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 25s 121ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.0461 - val_accuracy: 0.9884\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 25s 121ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.0547 - val_accuracy: 0.9865\n",
      "Epoch 10/10\n",
      "210/210 [==============================] - 25s 121ms/step - loss: 0.0143 - accuracy: 0.9951 - val_loss: 0.0491 - val_accuracy: 0.9878\n"
     ]
    }
   ],
   "source": [
    "modelpath = '../mnist_my_model/{epoch:002d}-{val_loss:.4f}.h5'\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath = modelpath,\n",
    "                                       save_best_only = True)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience = 10)\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 200,\n",
    "                   verbose = 1, validation_split=0.3,\n",
    "                   callbacks = [checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cf08abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0388 - accuracy: 0.9892\n",
      "[0.03878115862607956, 0.9891999959945679]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91f208f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YGL\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\YGL\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../mnist_onnx/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../mnist_onnx/', include_optimizer=False)          # pb 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c5c176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.9.3-py3-none-any.whl (435 kB)\n",
      "Collecting flatbuffers~=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from tf2onnx) (2.26.0)\n",
      "Requirement already satisfied: six in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.14.1 in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from tf2onnx) (1.17.3)\n",
      "Collecting onnx>=1.4.1\n",
      "  Downloading onnx-1.11.0-cp37-cp37m-win_amd64.whl (11.2 MB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ygl\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (3.2)\n",
      "Installing collected packages: onnx, flatbuffers, tf2onnx\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "Successfully installed flatbuffers-1.12 onnx-1.11.0 tf2onnx-1.9.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.17.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install -U tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f481141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YGL\\anaconda3\\lib\\runpy.py:125: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-05-04 16:15:18,170 - WARNING - ***IMPORTANT*** Installed protobuf is not cpp accelerated. Conversion will be extremely slow. See https://github.com/onnx/tensorflow-onnx/issues/1557\n",
      "2022-05-04 16:15:18,172 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-05-04 16:15:18,812 - INFO - Signatures found in model: [serving_default].\n",
      "2022-05-04 16:15:18,812 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-05-04 16:15:18,812 - INFO - Output names: ['dense_3']\n",
      "WARNING:tensorflow:From C:\\Users\\YGL\\anaconda3\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-05-04 16:15:18,947 - WARNING - From C:\\Users\\YGL\\anaconda3\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-05-04 16:15:22,014 - INFO - Using tensorflow=2.3.0, onnx=1.11.0, tf2onnx=1.9.3/1190aa\n",
      "2022-05-04 16:15:22,014 - INFO - Using opset <onnx, 9>\n",
      "2022-05-04 16:15:35,670 - INFO - Computed 0 values for constant folding\n",
      "2022-05-04 16:15:46,408 - INFO - Optimizing ONNX model\n",
      "2022-05-04 16:15:46,471 - INFO - After optimization: Cast -1 (1->0), Const +1 (9->10), Identity -6 (6->0), Reshape +1 (1->2), Transpose -5 (6->1)\n",
      "2022-05-04 16:15:46,477 - INFO - \n",
      "2022-05-04 16:15:46,477 - INFO - Successfully converted TensorFlow model mnist_onnx to ONNX\n",
      "2022-05-04 16:15:46,477 - INFO - Model inputs: ['conv2d_2_input']\n",
      "2022-05-04 16:15:46,477 - INFO - Model outputs: ['dense_3']\n",
      "2022-05-04 16:15:46,477 - INFO - ONNX model is saved at model_mnist.onnx\n"
     ]
    }
   ],
   "source": [
    "! python -m tf2onnx.convert --saved-model mnist_onnx --output model_mnist.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad8bac15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "def on_mouse(event, x, y, flags, param):\n",
    "    global oldx, oldy\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "        \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), 255, 20, cv2.LINE_AA)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x, y\n",
    "            \n",
    "def norm_digit(img):\n",
    "    # 무게 중심 좌표 추출\n",
    "    m = cv2.moments(img)\n",
    "    cx = m['m10'] / m['m00']\n",
    "    cy = m['m01'] / m['m00']\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # affine 행렬 생성\n",
    "    aff = np.array([[1, 0, w/2 - (cx + 0.5)], [0, 1, h/2 - (cy + 0.5)]], \n",
    "                   dtype=np.float32)\n",
    "    \n",
    "    # warpAffine을 이용해 기하학 변환\n",
    "    dst = cv2.warpAffine(img, aff, (0, 0))\n",
    "    return dst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNet('./model_mnist.onnx')\n",
    "\n",
    "if net.empty():\n",
    "    print('net load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "img = np.zeros((400, 400), np.uint8)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "cv2.setMouseCallback('image', on_mouse)\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    \n",
    "    if key == 27:\n",
    "        break\n",
    "        \n",
    "    elif key == ord(' '):\n",
    "        blob = cv2.dnn.blobFromImage(img, 1/255., (28, 28))\n",
    "        net.setInput(blob)\n",
    "        prob = net.forward()\n",
    "        \n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
    "        digit = maxLoc[0]\n",
    "        \n",
    "        print(f'{digit} ({maxVal*100:4.2f}%)')\n",
    "        \n",
    "        img.fill(0)\n",
    "        \n",
    "#         cv2.imshow('image', img)\n",
    "\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ba5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d5e28f4",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d368f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('../caffe/fig/')\n",
    "\n",
    "img_path = []\n",
    "\n",
    "for i in filename:\n",
    "    img_name = '../caffe/fig/' + i\n",
    "    img_path.append(img_name)\n",
    "\n",
    "if img_path is None:\n",
    "    print('image read failed')\n",
    "    sys.exit()\n",
    "\n",
    "# 딥러닝(이미 완료된) 모델 불러오기    \n",
    "model = '../caffe/bvlc_googlenet.caffemodel'       # 모델 데이터\n",
    "config = '../caffe/deploy.prototxt'                # 배치 데이터\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)              # dnn으로 읽기\n",
    "\n",
    "if net.empty():\n",
    "    print('network load failed')\n",
    "    sys.exit()\n",
    "\n",
    "# 딥러닝으로 알아낼 수 있는 품목을 리스트로 만들기\n",
    "classNames = []\n",
    "\n",
    "with open('../caffe/classification_classes_ILSVRC2012.txt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "\n",
    "idx = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # img_path는 경로로만 저장되어 있으므로 cv2.imread()로 이미지 읽어내기\n",
    "    img = cv2.imread(img_path[idx])\n",
    "    \n",
    "    # dnn에 훈련된 정보와 동일하게 맞추기(내가 가지고 있는 사진을 변형)\n",
    "    blob = cv2.dnn.blobFromImage(img, 1, (244, 244), (104, 117, 123), swapRB = False)\n",
    "\n",
    "    # blob로 맞춘 내용 net(모델, 배치 데이터를 dnn으로 정리한 것)에 넣기\n",
    "    net.setInput(blob)\n",
    "    # prob에 net의 자료 저장((1, 1000)의 shape으로 들어가 있음)\n",
    "    # prob는 각 품목별 확률이 저장되어 있다고 생각하면 편함\n",
    "    prob = net.forward()\n",
    "    \n",
    "    # prob를 평탄화하여 out에 저장\n",
    "    out = prob.flatten()\n",
    "\n",
    "    # 평탄화된 자료를 argmax를 이용해 가장 높은 확률을 가지는 품목을 선택(인덱스)\n",
    "    ClassId = np.argmax(out)\n",
    "\n",
    "    confidence = out[ClassId]\n",
    "    category = classNames[ClassId]\n",
    "\n",
    "    text = f'{category} ({confidence*100:4.2f} %)'\n",
    "\n",
    "    cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_COMPLEX,\n",
    "               1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('img', img)\n",
    "    \n",
    "    key = cv2.waitKey(1000)\n",
    "    \n",
    "    if key == 27:\n",
    "        break\n",
    "    elif key == ord('s'):\n",
    "        key = cv2.waitKey()\n",
    "        if key == ord('s'):\n",
    "            key = cv2.waitKey(1000)\n",
    "        elif key == 27:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    idx += 1\n",
    "    \n",
    "    if idx >= len(img_path):\n",
    "        idx = 0\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cbf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b450e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e3c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe09d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f59c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18530af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802e896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822cd2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8531aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86d152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff26ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
